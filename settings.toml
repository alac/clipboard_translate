# This is the settings file. To override values here, create a user.toml by coping this.

[ai_settings]
# The default service selected by the dropdown
# "Claude"
# "Gemini"
# "openai_1" ... "openai_4"  # Generic OpenAI Completions, used by most services
api = "openai_1"


# 'openai_#' connections are generic.
# request_url should end in either /completions or /chat/completions
# system_prompt is sent as a 'system message' for all requests in ChatCompletions mode
#   For Completions, it is prefixed to all messages
# preset_name is for oobabooga/text-generation-webui
# is_json_schema_strict should be true for oobabooga/text-generation-webui and TabbyAPI

[openai_1]
display_name = "oobabooga"
request_url = 'http://127.0.0.1:5000/v1/completions'
is_chat_completion = true
context_length = 4096
api_key = ""
system_prompt = ""
model = ""
preset_name = 'none'
is_json_schema_strict = false

[openai_2]
display_name = "oobabooga"
request_url = 'http://127.0.0.1:5000/v1/completions'
is_chat_completion = true
context_length = 4096
api_key = ""
system_prompt = ""
model = ""
preset_name = 'none'
is_json_schema_strict = false

[openai_3]
display_name = "oobabooga"
request_url = 'http://127.0.0.1:5000/v1/completions'
is_chat_completion = true
context_length = 4096
api_key = ""
system_prompt = ""
model = ""
preset_name = 'none'
is_json_schema_strict = false

[openai_4]
display_name = "oobabooga"
request_url = 'http://127.0.0.1:5000/v1/completions'
is_chat_completion = true
context_length = 4096
api_key = ""
system_prompt = ""
model = ""
preset_name = 'none'
is_json_schema_strict = false

[gemini_pro_api]
# You can get a free api key here: https://ai.google.dev/gemini-api/docs/api-key
api_key = ""
# Pick one of the values from https://ai.google.dev/gemini-api/docs/models/gemini
# Mind the quota limits if you're using a higher quality model
api_model = "gemini-3-flash-preview"
system_prompt = """Respond directly with only the requested information.
Do not add any conversational elements, greetings, or explanations.
Use examples provided as a guide and follow the pattern to complete the task."""

[claude_api]
api_key = "YOUR_ANTHROPIC_API_KEY"
model = "claude-3-7-sonnet-20250219"
system_prompt = "You are a helpful AI assistant."

[azure_tts]
# Azure has a generous speech synthesis free plan
# Follow the instructions here to setup an account: https://learn.microsoft.com/en-us/azure/ai-services/speech-service/get-started-text-to-speech?tabs=windows%2Cterminal&pivots=programming-language-python#prerequisites
speech_key = ""
speech_region = ""
# Provided JP voices are:
#ja-JP-NanamiNeural
#ja-JP-KeitaNeural
#ja-JP-AoiNeural
#ja-JP-DaichiNeural
#ja-JP-MayuNeural
#ja-JP-NaokiNeural
#ja-JP-ShioriNeural
#ja-JP-MasaruMultilingualNeural
speech_voice = "ja-JP-KeitaNeural"

[define]
define_prompt_filepath = "prompts/define_base_forms.txt"
temperature = 0.7
stopping_strings = ["</task>", "</|system|>", "</|", "<|"]

[translate]
translate_prompt_filepath = "prompts/translate.txt"
temperature = 0.7
stopping_strings = ["</english>", "</task>", "</analysis>", "</|", "<|", "<|end|>"]

[translate_cot]
# Aka 'With Analysis (CoT)'. The idea is to ask questions to direct the translation.
cot_prompt_filepath = "prompts/cot_translation.txt"
cot_examples_filepath = "prompts/examples/cot_translation_original.txt"
# Saving COT outputs requires save_cot_outputs to be true, and for the current provider to be in
# save_cot_outputs_ai_providers.
save_cot_outputs_ai_providers = ["Gemini", "OpenAI", "Oogabooga"]
save_cot_outputs = false
temperature = 0.7
stopping_strings = ["</english>", "</task>", "</analysis>", "</|", "</", "<|end|>"]

[define_into_analysis]
# Aka 'Define->Analysis'. The idea is to improve the quality of readings by supplying them from a trusted source.
# enable_jmdict_replacements changes the behavior of Define->Analysis to look up readings in JMDict.
# The first time this is ON, it will extract JMDict which is VERY slow.
enable_jmdict_replacements = false
combine_all_readings = false


[translate_best_of_three]
# Aka 'Best of Three'. The idea is ask for three different translations and (optionally) ask the AI to pick the best one.
# enable_validation asks the AI to judge which translation was best. Very sketchy.
enable_validation = false
first_temperature = 0.0
second_temperature = 0.5
third_temperature = 1.0

[q_and_a]
q_and_a_prompt_filepath = "prompts/q_and_a.txt"
temperature = 0.0
stopping_strings = ["</answer>", "</task>", "</|", "<|"]

[kanji_breakdown]
prompt_filepath = "prompts/kanji_breakdown.txt"
# You can select a specific model for this task, e.g., "openai_1", "Gemini", "Claude"
api_service = "Gemini"
temperature = 0.3
stopping_strings = ["</task>", "</|", "<|end|>"]

[ui]
# You can set the behavior of the translate, analyze and define buttons.
# 'Off' just AI translates the sentence
# 'Translate' just AI translates the sentence
# 'Best of Three' does 3 separate AI translations and optionally asks which is the best.
# 'With Analysis (CoT)' asks clarification questions before asking for a translations.
# 'Define->Analysis' is the above with an extra step to try to get more accurate readings.
# 'Post-Hoc Analysis' is 'Translate' then 'With Analysis (CoT)'
# 'Define' asks for a vocabulary list without a translation
# 'Define (without AI)' attempts to generate a vocabulary list without AI at all
translate_button_action = 'Translate'
analyze_button_action = 'With Analysis (CoT)'
define_button_action = 'Define'
auto_action = 'Off'
hide_thinking = true

[general]
# The number of previous clipboard values to send to the AI as context.
translation_history_length = 15
# An additional 'context' to send with the AI request.
# Use this to describe lines that will be translated.
# For a story, this might be a synopsis or a list of characters.
translation_context = """>STORY_INFO_START
Example story data. The main character is the protagonist.
>STORY_INFO_END
"""
rate_limit = 15 # this is the rate limit for gemini1.5-flash
# Experimental. Do not touch.
min_length_for_auto_behavior = 0
max_auto_triggers = 0
skip_duplicate_lines = false
include_lines_in_output_in_duplicate_set = false
enable_interrupt = true

[app]
hot_reload = true
